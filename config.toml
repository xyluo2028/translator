[provider]
name = "ollama"

[ollama]
host = "http://localhost:11434"
# Set to a model you have pulled in Ollama, e.g. "gpt-oss"
model = "gpt-oss"

[defaults]
source_lang = "auto"
target_lang = "ZH"
tone = "neutral"
explain_lang = "EN"
temperature = 0.2
